{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7a7e17e-2fa1-4863-b0ba-5ffcf317c6d1",
   "metadata": {},
   "source": [
    "1. Indsamling og indlæsning af tekstfiler\n",
    "\n",
    "    Kilde: New York City Year-End 2023 Enforcement Report (PDF)\n",
    "    Indlæsning af PDF: Bruger jeg PyPDF2 til at udtrække tekst fra PDF-filen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b5a6554-32fa-44ed-b5b9-5b2f321110f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " \n",
      " \n",
      " \n",
      "Crime and Enforcement Activity  \n",
      " in New York City  \n",
      "(Jan 1 – Dec 31 , 2023 ) \n",
      "Edward A. Caban  \n",
      "Police Commissioner   i Introduction  \n",
      " \n",
      " This report presents statistics on race/ethnicity compiled from the New York City \n",
      "Police Department’s records management system .  Crime Complaint Reports contain \n",
      "information on the crime victims’ race and ethnicity as recorded by the officer s or \n",
      "precinct clerical staff interviewing the victim or complainant.   The victim’s description of \n",
      "any non -arrested suspects will also be recorded on the crime complaint report.  Arrests \n",
      "made by the responding officers will be recorded on an arrest report a ssociated with the \n",
      "crime complaint report and used to initiate the booking system.  Once the information has \n",
      "been entered and “signed” off by a supervisor it is stored in the Department’s Enterprise \n",
      "Data Warehouse.  The data for this report were drawn from  the Data Warehouse.    \n",
      "Additional information was drawn from a separate H omicide  and\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        reader = PyPDF2.PdfReader(file)\n",
    "        for page in reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "# Udtrækning af tekst fra PDF'en\n",
    "nyc_report_text = extract_text_from_pdf('year-end-2023-enforcement-report.pdf')\n",
    "print(nyc_report_text[:1000])  # Udskriv de første 1000 tegn af teksten for at bekræfte udtrækning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47878daa-73e1-4956-867d-43aedfe39921",
   "metadata": {},
   "source": [
    "Forklaring af koden og hvordan den virker\n",
    "\n",
    "Import af PyPDF2 bibliotek: Vi importerer PyPDF2 biblioteket, som bruges til at arbejde med PDF-filer i Python.\n",
    "\n",
    "Definere extract_text_from_pdf funktionen:\n",
    "Åbning af PDF-filen: with open(pdf_path, 'rb') as file: åbner PDF-filen i binær læsetilstand.\n",
    "Oprettelse af PdfReader objekt: reader = PyPDF2.PdfReader(file) opretter et PdfReader objekt, som vi bruger til at læse PDF-filen.\n",
    "Løkke gennem alle sider: for page in reader.pages: itererer gennem alle sider i PDF-filen.\n",
    "Udtrækning af tekst: text += page.extract_text() udtrækker tekst fra hver side og tilføjer den til text variablen.\n",
    "\n",
    "Kalder funktionen: nyc_report_text = extract_text_from_pdf('/mnt/data/year-end-2023-enforcement-report.pdf') kalder funktionen og gemmer den udtrukne tekst i variablen nyc_report_text.\n",
    "\n",
    "Udskrivning af tekst: print(nyc_report_text[:1000]) udskriver de første 1000 tegn af den udtrukne tekst for at bekræfte, at teksten er blevet korrekt udtrukket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a84d349e-0e3f-4058-b464-1078a37f3b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Uddrag, rens og transformer tekst\n",
    "\n",
    "Tekstrensning: Fjern unødvendige tegn, tokeniser teksten, og forbered den til NLP-behandling.\n",
    "\n",
    "Teksttransformation: Konverter den rensede tekst til vektorer ved hjælp af TF-IDF eller word embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "726534d3-aa99-4814-8367-3bf66eb60df3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jeffo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jeffo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Download nødvendige NLTK datafiler\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Definer funktion til at rense og tokenisere tekst\n",
    "def preprocess_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    cleaned_text = ' '.join([word for word in tokens if word.lower() not in stop_words and word.isalnum()])\n",
    "    return cleaned_text\n",
    "\n",
    "# Anvend funktionen på den udtrukne tekst fra PDF'en\n",
    "cleaned_text = preprocess_text(nyc_report_text)\n",
    "\n",
    "# Initialiser TF-IDF vektorisering og anvend det på den rensede tekst\n",
    "vectorizer = TfidfVectorizer()\n",
    "text_vectors = vectorizer.fit_transform([cleaned_text])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "394469f5-ffa9-4f65-bdcf-0d6b75912cfc",
   "metadata": {},
   "source": [
    "Import af nødvendige biblioteker\n",
    "\n",
    "Jeg importerer nltk til naturlig sprogbehandling og sklearn til tekstvektorisering.\n",
    "nltk.download('punkt') og nltk.download('stopwords') sikrer, at nødvendige NLTK-datafiler er downloadet.\n",
    "\n",
    "2. Definere funktionen preprocess_text\n",
    "\n",
    "Funktionen tager en tekststreng som input.\n",
    "Tokenisering: word_tokenize(text) splitter teksten op i individuelle ord (tokens).\n",
    "Stopord: stop_words = set(stopwords.words('english')) indlæser en liste over almindelige ord (som 'the', 'and'), der fjernes fra teksten.\n",
    "Rensning af tekst: Kun ord, der ikke er stopord og er alfanumeriske, bevares: [word for word in tokens if word.lower() not in stop_words and word.isalnum()].\n",
    "\n",
    "Den rensede tekst sættes sammen igen til en enkelt streng.\n",
    "\n",
    "3. Udførelse af tekstrensning og vektorisering\n",
    "\n",
    "cleaned_text = preprocess_text(nyc_report_text) anvender funktionen på teksten udtrukket fra PDF'en.\n",
    "vectorizer = TfidfVectorizer() initialiserer en TF-IDF vektorisering, som vægter ord baseret på deres hyppighed i dokumentet.\n",
    "text_vectors = vectorizer.fit_transform([cleaned_text]) anvender TF-IDF vektorisering på den rensede tekst, hvilket resulterer i numeriske vektorer, som kan bruges til yderligere analyse."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d641b215-e8d8-4614-831f-5c4a46d561cc",
   "metadata": {},
   "source": [
    "Trin 3: Opbevaring af transformerede dokumenter\r",
    ":Jeg\n",
    "Vi bruger numpy til at gemme og hente de transformerede vektorer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "777210a5-4813-42ee-b0e5-db6d7f4b3741",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Konvertering af vektorer til numpy-array\n",
    "vectors = text_vectors.toarray()\n",
    "\n",
    "# Gemme vektorerne i en fil\n",
    "np.save('text_vectors.npy', vectors)\n",
    "\n",
    "# Hente vektorerne fra filen\n",
    "loaded_vectors = np.load('text_vectors.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923eb087-ba46-4c98-9984-f1c4182db4fd",
   "metadata": {},
   "source": [
    "numpy er et bibliotek, der er bredt brugt til numeriske beregninger i Python. Det giver os mulighed for at gemme de transformerede tekstvektorer som filer, hvilket gør det nemt at hente og genbruge dem senere."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60fcd2b7-88fc-4dd9-b08d-1ac15d56e5d9",
   "metadata": {},
   "source": [
    "Anvendelse af indhold til at forbedre en forudtrænet LLM\n",
    "\n",
    "Jeg bruger transformers biblioteket fra Hugging Face til at finjustere en forudtrænet GPT-2 model.\n",
    "\n",
    "Jeg deler teksten op i mindre stykker for at undgå længdebegrænsningen og finjusterer modellen med disse stykker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e379b09-4aa7-4614-b1c3-6c07923c0bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finetuning complete.\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2Tokenizer, GPT2LMHeadModel\n",
    "import torch\n",
    "\n",
    "# Indlæsning af tokenizer og model\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Forberedelse af input data\n",
    "# Del teksten op i stykker på maksimalt 1024 tokens\n",
    "max_length = 1024\n",
    "input_texts = [cleaned_text[i:i+max_length] for i in range(0, len(cleaned_text), max_length)]\n",
    "\n",
    "# Indstilling af model til træningstilstand\n",
    "model.train()\n",
    "\n",
    "# Træning med batch af input stykker\n",
    "for input_text in input_texts:\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n",
    "    outputs = model(input_ids, labels=input_ids)\n",
    "    loss = outputs.loss\n",
    "    loss.backward()\n",
    "\n",
    "print(\"Finetuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e0cac5-84c2-4ca2-b6e7-0f3d652a8c50",
   "metadata": {},
   "source": [
    "Hvorfor jeg bruger det:\r\n",
    "  Opdeling af tekstJegVi deler teksten op i stykker, så de passer til modellens maksimale sekvenslængde. Dette sikrer, ajegvi undgår længdebegrænsningsfejle\n",
    "n   Batch træninJeg Vi træner modellen med hver af disse stykker for at sikre, at hele teksten bruges til finjustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8acfe56e-a7ba-4153-bbcd-65443a72aa16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
